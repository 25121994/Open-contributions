{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haar Classifier with OpenCv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement haar classifier, first cv2 is needed to be imported..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - the CascadeClassifier class to detect objects in a video stream<br>\n",
    "We'll need to extract the xml file containing the classifier, in this case we are extracting face classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each frame image, the image will be converted to gray scale image than detect the image.<br>\n",
    "- <b>cv2.CascadeClassifier.detectMultiScale</b> - Detects objects of different sizes in the input image. The detected objects are returned as a list of rectangles in this case <i>'faces'</i>.<br>\n",
    "- Than all the rectangles are read in the loop and created, with <b>cv2.rectangle</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_classification(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # converting to gray-scale\n",
    "    \n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4) # perform the detection  \n",
    "    \n",
    "    for (x, y , w ,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>cv2.VideoCapture(index)</b> starts video capture, where <i>index</i> is location of the video, for video cam, <i>index=0</i> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While video is captured, image is constantly extracted and processed in <i>face_recognition</i>, then showed using <b>cv2.imshow(tag,img)</b><br>\n",
    "<b>cv2.waitKey</b> - is a command to close the while loop with an external event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    # Reading one image frame from vedio capture instance\n",
    "    _, img = cap.read()\n",
    "    \n",
    "    # Call the method\n",
    "    face_classification(img)\n",
    "\n",
    "    # Display the output\n",
    "    cv2.imshow('img', img)\n",
    "    # Close if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
